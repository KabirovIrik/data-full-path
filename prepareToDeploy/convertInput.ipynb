{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import joblib, pickle\n",
    "import pymorphy2\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stopwords = set(nltk_stopwords.words('russian'))\n",
    "my_stopwrods = ['а', 'будем', 'будет', 'будете', 'будешь', 'буду', 'будут', 'будучи', 'будь', 'будьте', 'бы', 'был', 'была', 'были', 'было', 'быть', 'в', 'вам', 'вами', 'вас', 'весь', 'во', 'вот', 'все', 'всё', 'всего', 'всей', 'всем', 'всём', 'всеми', 'всему', 'всех', 'всею', 'всея', 'всю', 'вся', 'вы', 'да', 'для', 'до', 'его', 'едим', 'едят', 'ее', 'её', 'ей', 'ел', 'ела', 'ем', 'ему', 'емъ', 'если', 'ест', 'есть', 'ешь', 'еще', 'ещё', 'ею', 'же', 'за', 'и', 'из', 'или', 'им', 'ими', 'имъ', 'их', 'к', 'как', 'кем', 'ко', 'когда', 'кого', 'ком', 'кому', 'комья', 'которая', 'которого', 'которое', 'которой', 'котором', 'которому', 'которою', 'которую', 'которые', 'который', 'которым', 'которыми', 'которых', 'кто', 'меня', 'мне', 'мной', 'мною', 'мог', 'моги', 'могите', 'могла', 'могли', 'могло', 'могу', 'могут', 'мое', 'моё', 'моего', 'моей', 'моем', 'моём', 'моему', 'моею', 'можем', 'может', 'можете', 'можешь', 'мои', 'мой', 'моим', 'моими', 'моих', 'мочь', 'мою', 'моя', 'мы', 'на', 'нам', 'нами', 'нас', 'наса', 'наш', 'наша', 'наше', 'нашего', 'нашей', 'нашем', 'нашему', 'нашею', 'наши', 'нашим', 'нашими', 'наших', 'нашу', 'не', 'него', 'нее', 'неё', 'ней', 'нем', 'нём', 'нему', 'нет', 'нею', 'ним', 'ними', 'них', 'но', 'о', 'об', 'один', 'одна', 'одни', 'одним', 'одними', 'одних', 'одно', 'одного', 'одной', 'одном', 'одному', 'одною', 'одну', 'он', 'она', 'оне', 'они', 'оно', 'от', 'по', 'при', 'с', 'сам', 'сама', 'сами', 'самим', 'самими', 'самих', 'само', 'самого', 'самом', 'самому', 'саму', 'свое', 'своё', 'своего', 'своей', 'своем', 'своём', 'своему', 'своею', 'свои', 'свой', 'своим', 'своими', 'своих', 'свою', 'своя', 'себе', 'себя', 'собой', 'собою', 'та', 'так', 'такая', 'такие', 'таким', 'такими', 'таких', 'такого', 'такое', 'такой', 'таком', 'такому', 'такою', 'такую', 'те', 'тебе', 'тебя', 'тем', 'теми', 'тех', 'то', 'тобой', 'тобою', 'того', 'той', 'только', 'том', 'томах', 'тому', 'тот', 'тою', 'ту', 'ты', 'у', 'уже', 'чего', 'чем', 'чём', 'чему', 'что', 'чтобы', 'эта', 'эти', 'этим', 'этими', 'этих', 'это', 'этого', 'этой', 'этом', 'этому', 'этот', 'этою', 'эту', 'я']\n",
    "all_stopwords = set(my_stopwrods).union(stopwords)\n",
    "replace_city_name = 'название_город' # название для замены\n",
    "USE_SELENIUM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные:\n",
    "- html-текст\n",
    "- Поисковый запрос\n",
    "- Город"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SELENIUM:\n",
    "    chromedriver = 'd:/games/chrome/chromedriver.exe'\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "    options.add_argument('headless')  # для открытия headless-браузера\n",
    "    browser = webdriver.Chrome(executable_path=chromedriver, chrome_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input = 'керамзитобетонные блоки в Уфе'\n",
    "lr = 172\n",
    "url = 'https://ufa.promportal.su/tags/8433/bloki-keramzitobetonnie/'\n",
    "if USE_SELENIUM:\n",
    "    browser.get(url)\n",
    "    my_file = browser.page_source\n",
    "else:\n",
    "    my_file = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q = pd.read_csv('queries_full.csv', sep=';')\n",
    "query = df_q[df_q['search_query'] == query_input]\n",
    "df_cities = pd.read_csv('lr_regions.csv', encoding='UTF-8', sep=';')\n",
    "df_lr = df_cities[df_cities['lr'] == str(lr)]\n",
    "city_name = df_lr['name1'].values[0]\n",
    "all_cities = df_lr['name2'].to_list() + df_lr['name1'].to_list() # список городов в Им.падеже в конце\n",
    "with open('qu_idf.pkl', 'rb') as f:\n",
    "    qu_idf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовим текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceCityName(val):\n",
    "    for city in all_cities:\n",
    "        val = val.lower().replace(city.lower(), replace_city_name)\n",
    "    return val\n",
    "\n",
    "def getTextContent(val):\n",
    "    soup = BeautifulSoup(val, 'lxml')\n",
    "    return soup.body.get_text(separator=\" \", strip=True)\n",
    "\n",
    "def get_lemm(text):\n",
    "    \"\"\"\n",
    "    Подготив строки, токенизируем и проведем лемматизацию\n",
    "    \"\"\"\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    return ' '.join([morph.parse(w)[0].normal_form for w in word_list])\n",
    "\n",
    "my_file = replaceCityName(my_file)\n",
    "textContent = get_lemm(getTextContent(my_file))\n",
    "#my_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получаем фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len_mt, len_md, len_kw, len_w_mt, len_w_md, len_w_kw\n",
    "def getMeta(val):\n",
    "    \"\"\"\n",
    "    val - html docuemnt\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(val, 'lxml')\n",
    "    try:\n",
    "        word_list = nltk.word_tokenize(soup.head.title.text.replace('\\xa0', ' '))\n",
    "        title = ' '.join([morph.parse(w)[0].normal_form for w in word_list])\n",
    "    except:\n",
    "        title = ''\n",
    "        \n",
    "    try:\n",
    "        word_list = nltk.word_tokenize(soup.head.find_all(attrs={\"name\" : \"description\"})[0]['content'].replace('\\xa0', ' '))\n",
    "        descr = ' '.join([morph.parse(w)[0].normal_form for w in word_list])\n",
    "    except:\n",
    "        descr = ''\n",
    "    try:\n",
    "        word_list = nltk.word_tokenize(soup.head.find_all(attrs={\"name\" : \"keywords\"})[0]['content'].replace('\\xa0', ' '))\n",
    "        kw = ' '.join([morph.parse(w)[0].normal_form for w in word_list])\n",
    "    except:\n",
    "        kw = ''\n",
    "    return title.lower(), descr.lower(), kw.lower(), len(title), len(descr), len(kw)\n",
    "\n",
    "def getLenWords(val):\n",
    "    return len(val.split(' '))\n",
    "\n",
    "def getLenKeyWords(val):\n",
    "    cnt_semi = val.count(',')\n",
    "    return cnt_semi + ((cnt_semi > 0) * 1) or (len(val) > 0) * 1\n",
    "\n",
    "# words_count, words_count_sw\n",
    "def getWordsCount(text):\n",
    "    word_list = tokenizer.tokenize(text) # с стоп-словами\n",
    "    word_list_sw = [w for w in word_list if not w in all_stopwords] # без стоп-слов\n",
    "    return len(word_list), len(word_list_sw)\n",
    "\n",
    "# spamity, max_spam\n",
    "def getSpamity(val):\n",
    "    # получаем список слов и исключаем стоп слова\n",
    "    word_list = [w for w in tokenizer.tokenize(val) if not w in all_stopwords] \n",
    "    cnt_words = Counter(word_list)\n",
    "    max_freq = cnt_words.most_common(1)[0][1]\n",
    "    return max_freq/len(word_list), max_freq\n",
    "\n",
    "# water_content\n",
    "def getWaterContent(val, words_count):\n",
    "    \"\"\"\n",
    "    Количество вхождение стоп слов в общее количество слов\n",
    "    \"\"\"\n",
    "    word_list = tokenizer.tokenize(val)\n",
    "    cnt_stopwords = sum(map(word_list.count, all_stopwords)) # количество вхождений стопслов в контент\n",
    "    return cnt_stopwords/words_count\n",
    "\n",
    "# density\n",
    "def getDensityApply(val, qu, replace_city=True):\n",
    "    txt = val.lower()\n",
    "    qu = qu.lower()\n",
    "    if replace_city:\n",
    "        # обрезаем вхождение города\n",
    "        city_name = replace_city_name\n",
    "        qu = qu.replace(' в ' + city_name, '')\n",
    "        qu = qu.replace(' ' + city_name, '')\n",
    "    # разбиваем текст\n",
    "    word_list = [w for w in tokenizer.tokenize(txt) if not w in all_stopwords]\n",
    "    word_cnt = len(word_list)\n",
    "    #print(word_list)\n",
    "    #lemms = [morph.parse(w)[0].normal_form for w in word_list]\n",
    "    qu_arr = np.array([])\n",
    "    # каждое слово в запросе считаем отдельно\n",
    "    for q in qu.split():\n",
    "        qu_arr = np.append(qu_arr, len(re.findall(rf'{q}', txt)))\n",
    "    cnt_qu = qu_arr.mean()\n",
    "    #return cnt_qu/word_cnt, len(re.findall(rf'{qu}', txt))/word_cnt\n",
    "    return cnt_qu/word_cnt\n",
    "\n",
    "\n",
    "# tf_idf  <<================      поправить\n",
    "def getTFIDF(density, qu):\n",
    "    url = row['url']\n",
    "    q = row['search_query_n']\n",
    "    tf = df[df['url'] == url][q+'_density'].values[0]\n",
    "    idf = qu_idf[q]\n",
    "    return tf/idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'freq',\n",
    "- 'query_results_count_num',\n",
    "- 'len_mt',\n",
    "- 'len_md',\n",
    "- 'len_kw',\n",
    "- 'len_w_mt',\n",
    "- 'len_w_md',\n",
    "- 'len_w_kw',\n",
    "- 'words_count',\n",
    "- 'words_count_sw',\n",
    "- 'spamity',\n",
    "- 'max_spam',\n",
    "- 'water_content',\n",
    "- 'tf_idf',\n",
    "- 'density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input_common = ' '.join([morph.parse(w)[0].normal_form for w in tokenizer.tokenize(query_input)]).replace(city_name.lower(), replace_city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model = {}\n",
    "data_to_model['freq'] = query['freq'].values[0] \n",
    "data_to_model['query_results_count_num'] = query['query_results_count_num'].values[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, descr, kw, data_to_model['len_mt'], data_to_model['len_md'], data_to_model['len_kw']   = getMeta(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model['len_w_mt'] = getLenWords(title)\n",
    "data_to_model['len_w_md'] = getLenWords(descr)\n",
    "data_to_model['len_w_kw'] = getLenKeyWords(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model['words_count'], data_to_model['words_count_sw'] = getWordsCount(textContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_model['spamity'], data_to_model['max_spam'] = getSpamity(textContent)\n",
    "data_to_model['water_content'] = getWaterContent(textContent, data_to_model['words_count'])\n",
    "data_to_model['density'] = getDensityApply(textContent, query_input_common, replace_city=False)\n",
    "data_to_model['tf_idf'] = data_to_model['density']/qu_idf[query_input_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': 410,\n",
       " 'query_results_count_num': 9000,\n",
       " 'len_mt': 97,\n",
       " 'len_md': 133,\n",
       " 'len_kw': 108,\n",
       " 'len_w_mt': 13,\n",
       " 'len_w_md': 18,\n",
       " 'len_w_kw': 1,\n",
       " 'words_count': 1476,\n",
       " 'words_count_sw': 1292,\n",
       " 'spamity': 0.047987616099071206,\n",
       " 'max_spam': 62,\n",
       " 'water_content': 0.12466124661246612,\n",
       " 'density': 0.07352941176470588,\n",
       " 'tf_idf': 0.0900954844478508}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['freq', 'query_results_count_num', 'len_mt', 'len_md', 'len_kw',\n",
    "       'len_w_mt', 'len_w_md', 'len_w_kw', 'words_count', 'words_count_sw',\n",
    "       'spamity', 'max_spam', 'water_content', 'tf_idf', 'density']\n",
    "predict_data = np.array([data_to_model[col] for col in list_columns]).reshape(1, -1)\n",
    "#predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keramzitobetonnyi_blok_v_nazvanie_gorod_rf.sav\n"
     ]
    }
   ],
   "source": [
    "with open('../app/query_dict_data.pkl', 'rb') as f:\n",
    "    query_data = pickle.load(f)[query_input_common]\n",
    "    rf_model = joblib.load('../app/'+query_data['model_rf'])\n",
    "    gbc_model = joblib.load('../app/'+query_data['model_gbc'])\n",
    "#rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_rf': 'keramzitobetonnyi_blok_v_nazvanie_gorod_rf.sav',\n",
       " 'model_gbc': 'keramzitobetonnyi_blok_v_nazvanie_gorod_gbc.sav',\n",
       " 'borders': {'spamity': [0.036960199951196405, 0.06522276392780524],\n",
       "  'water_content': [0.11359012780261948, 0.16872470536433645],\n",
       "  'tf_idf': [0.016496229232281424, 0.04689328765901235],\n",
       "  'density': [0.01346302802209233, 0.03827090645454176]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "99\n",
      "89.5\n"
     ]
    }
   ],
   "source": [
    "rf_pred = np.int(rf_model.predict_proba(predict_data)[0][0] * 100)\n",
    "gbc_pref = np.int(gbc_model.predict_proba(predict_data)[0][0] * 100)\n",
    "res_pred = (rf_pred + gbc_pref) / 2\n",
    "print(rf_pred)\n",
    "print(gbc_pref)\n",
    "print(res_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
