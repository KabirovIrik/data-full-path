# Анализ данных

### Сбор данных и создание текстовых характеристик <a href="concatDf.ipynb">concatDf.ipynb</a>

- Собраны в одну таблицу данные по поисковым запросам и контент страниц сайтов
- Контент страниц лемматизирован
- Для каждой страницы выделены метатеги, посчитаны длина в символах и в словах этих метатегов
- Для тега body посчиатны водность и спамность текста
- Для каждой связки "Поисковый запрос" - "Контент страницы" расчитаны частота вхождения поискового запроса в контент страницы, а также TF-IDF 

### Выбор ML-модели <a href="procced_train.ipynb">procced_train.ipynb</a>

- В качестве целевого значения для модели было выбрано: попадёт ли страница по нужному запросу на первую страницу поисковой выдачи, т.е. задача бинарой классификации 
- В качестве моделей используются Случайный лес и Градиентный бустинг: берется их средний прогноз (В процессе: возможно опорные вектора улучшат две предыдущих модели)
- Значение метрики ROC_AUC_SCORE варьируется от 68% до 79% для отдельных запросов

<a href='../collect'>Назад  - сбор данных</a> <br>
<a href='../prepareToDeploy'>Вперед - подготовка к размещению</a>
